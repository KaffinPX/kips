```
  KIP: 5
  Layer: Consensus (hard fork), Block Validation
  Title: Proofs of Validation (PoV) and Proofs of Chain Membership (PoChM)
  Author: Shai Wyborski <shai.wyborski@mail.huji.ac.il>
  Status: proposed
```

# Motivation
The pruning mechanism makes it impossible to prove that a transaction was included in the ledger after it has been pruned. The only currently available solution is publicly run archival nodes (in the form of block explorers) that store all historical data. However, this is not a sustainable solution, since the size of the databases increases rapidly with time and adoption.

A better solution is to provide a *cryptographically verifiable proof* that a transaction was posted to the blockchain. Such a proof could only be *generated* before the transaction was pruned (or by using an archival node), but could be *verified* indefinitely.

We consider two types of proof: a *proof of publication* (PoP) and a *proof of validation* (PoV). The first kind only proves that a transaction was present on the blockDAG at some point, whereas the second proves that a transaction was *accepted*. That is, while a proof of publication can prove that ``txn`` was posted to the blockchain, it can not discount the scenario where a *conflicting* transaction ``txn'`` was also posted and that ``txn'`` was validated while ``txn`` was discarded.

In this KIP I will mostly concentrate about proofs of validation. It is possible to obtain a PoP that is slightly smaller, but the difference is so marginal I propose to use a PoV even in contexts where a PoP is sufficient.

The current state of the art is that it is technically possible to obtain proofs of validation, but such proofs can grow rather large (up to about 24 megabytes in the worst case), and there is no API call for generating such a proof. I describe a small modification to the block validation rules that extremely reduces the size of a proof of validation (down to about 9.5 kilobytes in the worst case) while incurring very mild costs on the performance of the network (in terms of header size and block validation complexity). I also provide a precise algorithmic descriptions of how such a proof is generated and verified, with the intention that they will be implemented as API calls.

I stress that the current sizes are calculated with respect to the values of various parameters in the current 1BPS consensus. Changing these parameters would require recalculating these values. However, increasing block rates will only increase the factor by which proposed proofs are improve upon currently possible proofs (roughly because currently possible proofs are as large as the entire ledger stored between two consecutive pruning blocks, whereas the size of proposed proofs grows *logarithmically* with the number of *chain blocks* between two consecutive pruning blocks. In particular, increasing BPS will increase the size of current proofs, but not the size of proposed proofs).

# Notations

In this proposal, it is convenient to use the notation ``Past(B)`` (resp. ``Future(B)``) to denote the past (resp. future) of the block ``B`` *including* the block ``B`` itself. The method names are capitalized to differentiate them from the common notations ``past(B)`` and ``future(B)`` which *exclude* the block ``B`` itself.

I use the notation ``parent(B,n)`` to note the *nth selected parent* of ``B``. For brevity, I use ``parent(B)`` instead of ``parent(B,1)``. For any ``n>1`` we can recursively define ``parent(B,n)=parent(parent(B,n-1))``.

# Posterity Headers

The consensus state of a Kaspa node includes a list of selected chain block headers. These headers are sampled at (approximate) regular intervals and are stored indefinitely. Hence, we call them *posterity headers*.

Currently, posterity headers are taken from blocks used as pruning blocks, and a posterity header is stored once every 24 hours, whereby they are also commonly referred to as *pruning headers*. However, in this proposal I propose to decouple posterity from pruning in order to increase the density of posterity headers. 

Given a block ``B`` let ``posterity(B)`` be the *earliest* posterity header such that ``B ∈ Past(posterity(B))``, or ``null`` if such a stored header does not exist yet. Let ``posterity_depth(B)`` output the integer ``n`` satisfying ``B=parent(posterity(B),n)``.

For any block ``B`` let ``next_posterity(B)`` be the block header with the following property: if ``B`` is a posterity header, then ``next_posterity(B)`` is the next stored header. Note that this is well-defined even if ``B`` is not a posterity header.

Posterity headers have the following important properties:
 * They are determined in consensus. That is, all nodes store the same posterity headers.
 * They are headers of blocks in the *selected chain*.
 * Each block ``B`` contains a pointer to ``posterity(posterity(posterity(B)))``. Hence, the chain of posterity headers is verifiable all the way down to genesis. In particular, obtaining and verifying the posterity chain is part of the process of syncing a new node.

The reason that ``B`` points to ``posterity(posterity(posterity(B)))`` and not is ``posterity(B)`` that the original motivation for storing these blocks comes from the pruning mechanism, where these depth-3 pointers are required.

# Accepted Transactions Merkle Root (ATMR)

In Bitcoin, every header contains the root of a Merkle tree of all transactions included in this block. In Kaspa, this Merkle tree is extended to contain all *accepted* transactions included in this block *and its merge set*. That is, all transactions that appear either in the block or the merge set of the block, except transactions that conflict other transactions that precede them in the GHOSTDAG ordering. 

# Proofs of Chain Membership (PoChM)

To provide a proof that ``txn`` was validated, it suffices to provide the following:
 * A header of a block ``B`` and a Merkle proof that ``txn`` appears in ``B``'s (ATMR)
 * Proof that ``B`` appeared in the selected chain

This suffices to prove that ``txn`` was validated even if a conflicting transaction ``txn'`` (or any number thereof) was also included in the blockDAG: the validation rules imply that ``txn`` and ``txn'`` both appeared in the anticone of ``B``, and that ``txn`` preceded ``txn'`` in the GHOSTDAG ordering.

The first item is a straightforward Merkle proof. However, the second item is trickier. I refer to such a proof as a *proof of chain membership* (PoChM, pronounced like the Latin word *pacem*) for ``B``. The rest of this document is concerned with providing a PoChM for an arbitrary block ``B``.

# PoChM Without Hard-Fork

Currently, the most straightforward way to construct a PoChM for ``B`` is to store the entire set ``Future(B) ∩ Past(posterity(B))``. The result is a "diamond shaped" DAG whose top block is ``posterity(B)`` and bottom block is ``B``. Given a DAG of this shape as proof, any node could verify that the top block is a posterity block, and that following the selected parent from the top block leads to the bottom block.

The problem with this proof is its size. In the worst case, it would be about as large as 24 hours worth of headers. At the current 1BPS and header size of 248 bytes, this sums to about 24 megabytes.

Remark: This could be improved slightly by, instead of storing the entire set, only storing the headers of the selected chain blocks and their parents. This data suffices to compute the selected parent of each selected chain block and validate the proof. However, this does not seem to improve the size by much. Also note that proofs for many transactions that were accepted in chain blocks with the same ``posterity`` can be aggregated. In particular, a PoChM for a block ``B`` is also a PoChM for any chain block ``C ∈ Future(B) ∩ Past(posterity(B))``

# Performance trade-offs

We aim to decrease the size of a PoChM as much as possible while minimizing performance costs. There are two relevant types of costs: header sizes, and block validation complexity.

As an extreme example, one could provide a very small PoChM by including in each posterity header the entire list of headers of all chain blocks down to the next posterity header. This "solution" is obviously prohibitive as it will make block headers huge.

On the other extreme, one could include in each header a *Merkle tree* of all chain blocks down to the next posterity header, and let the PoChM of B be a Merkle proof for that tree. While this "solution" only increases the size of a block header by 32 bytes (the size of a single hash), it makes it necessary to compute tens of thousands of hashes to validate a single block header, which is prohibitively costly.

Our proposal "balances" the second approach: we add to each header the root of a Merkle tree that only contains *logarithmically many* headers. This allows generating a PoChM in the form of a logarithmically long chain of headers and Merkle proofs.

In the current parametrization, implementing our proposal requires increasing the size of a header by a single hash (32 bytes), and adds a validation step with constant space complexity and a time complexity of θ(log(N)) where N is the number of *chain* blocks between two consecutive posterity blocks. The size of a PoChM, as well as the time required to verify it, is θ(log(N)loglog(N)).

We will provide non-asymptotic bounds after specifying the solution. For now we will state that in the current parametrization (without increasing posterity header density), the new block validation step requires computing 33 hashes (and can be skipped for blocks outside the selected chain), and that, in the worst case, the size of a PoChM is about 9.5 kilobytes.

# Our Proposal

The block header will contain a new field called the *PoChM Merkle root* (PMR) defined as follows: let k be the least integer such that ``parent(B,2^k) ∈ Past(next_posterity(B))``, then PMR is the root of the Merkle tree containing the headers ``parent(B,2^i)`` for ``i = 0,...,k-1``.

Let ``PMR(B,i)`` be the function that outputs a Merkle proof that ``hash(parent(B,2^i))`` is in the tree whose root is the PMR of B.

The process of header validation of chain block candidates will include verifying the PMR. I propose that the PMR will not be validated for blocks that are not chain candidates. In particular, a block whose PMR is invalid but is otherwise valid will remain in the DAG but will be disqualified from being a selected tip/parent. A similar approach is employed e.g. when validating UTXO commitments or checking that the block does not contain double spends. (A crucial subtlety is that, eventually, the selected parent of a block is *always* the parent with the highest blue accumulated work (BAW). If while validating the selected parent the block turns out to be disqualified, then the pointer to this block is *removed*. This rule allows us to read the selected parent of any block from the headers of its parents alone, without worrying about the parent with the highest BAW is disqualified for some external reason that requires additional data to notice).

The procedure for generating a *PoChM* for a block ``B`` is as follows:
     
    Let C = posterity(B)
    If C=null:
         Return error
    Let d = posterity_depth(B)
    Let proof = []
    While true:
          Let i = floor(log_2(d))
          proof.append(PMR(C,i))
          d -= 2^i
          If d == 0:
              Break
          C = parent(C,2^i)
          proof.append(C)
     Return proof

To understand how to validate the proof we first consider it in two simple cases:

If there is some i such that ``posterity_depth(B) = 2^i`` then ``B`` itself is a member of the PMR of ``posterity(B)`` and the entire PoChM is a single Merkle proof.

If there are some i>j such that ``posterity_depth(B) = 2^i + 2^j`` then the proof would contain three items:
 * A Merkle proof that ``hash(parent(posterity(B),2^i))`` is in the PMR of ``posterity(B)``
 * The header ``parent(posterity(B),2^i)`` (that in particular includes its PMR)
 * A Merkle proof that ``hash(B)`` is in the PMR of ``parent(posterity(B),2^i)``

By verifying the proofs and hashes above, one verifies that ``B`` is indeed a chain block. The general procedure extends similarly.

# Posterity Header Density

As a final optimization, I propose increasing the stored block density to once an hour. The main motivation for this optimization is to reduce the time required before a PoChM could be generated. A prerequisite for generating a PoChM is that stored_header(B) already exists, and reducing this time is beneficial. Additionally, this would meaningfully reduce (by around 40%) the complexity of the added verification step and of PoChM verification and the size of a PoChM.

However, this introduces additional costs. Currently, posterity headers double as pruning headers. Decoupling them from each other means that an additional ``posterity header`` would have to be added, increasing the header size to 312 bytes. In addition, this decoupling is tricky engineering wise and is probably more complicated to implement than the entire rest of the KIP.

I recommend first implementing this KIP using the current posterity/pruning header, and deciding on separate posterity headers with increased density later, based on demand and necessity. It might also be the case that the additional pointer might be removed (i.e. the pruning mechanism will somehow piggyback on the posterity blocks in a way that doesn't have computational costs), that should be subject of a separate discussion.

# Size of PoChM

Computing the actual size of a PoChM is a bit tricky, as it depends on the number of *chain* blocks between ``C`` and ``next_posterity(C)`` for several blocks ``C``. Buy staring at the KGI for sufficiently long, one can get convinced that the selected chain grows by about one block every two seconds (that is, in 1BPS we see that about half of the blocks are chain blocks). To provide a reliable upper bound, I will assume that the selected chain grows at a rate of about 1 block per second. Note that the growth of the selected chain is not governed by block rates, but by network conditions. Hence, I assume this holds with overwhelming probability for any block rate. This might not hold if network conditions improve greatly. However, we'll soon see that the growth asymptotics (as a function of a number of chain blocks between two consecutive posterity blocks) are a very mild log*loglog, this is hardly a concern. I will demonstrate this with concrete numbers after we obtain an expression for the size of a PoChM.

Let ``L_e`` and ``L_a`` denote the size of a header and a hash, respectively. Let ``N`` be the number of seconds between two consecutive posterity blocks. For a block ``B`` let ``|B|`` denote the Hamming weight of the binary representation of ``posterity_depth(B)``. It follows that a PoChM for ``B`` contains ``|B|`` Merkle proofs and ``|B|-1`` headers (in particular, if ``B=posterity(B)`` (equivalently ``posterity_depth(B)=0``) then ``|B|=0``, and indeed the "proof" is empty, since the consensus data itself proves that ``B`` is a chain block).

The size of each merkle proof is ``(2*log(logN))+1)*L_a``, so the total size of the PaChM is ``(2*log(logN))+1)*L_a|B| + L_e*(|B|-1)``. In the worst case, we have that ``|B| = log(N)`` so we obtain a bound of ``(2*log(logN))+1)*logN*L_a + L_e*(logN-1)``. Assuming ``L_e=32 Bytes`` and ``L_a=280 Bytes`` and ``N=86400`` (that is, that a posterity block is sampled once every 24 hours), this comes up to 9 kilobytes.

Increasing posterity density to once per hour (that is, setting N=3600) would decrease the largest PoChM size to 6 kilobytes.

If network conditions improve so much that the selected chain grows at a rate of 10 blocks per second (which is unlikely to happen in the foreseeable future), the largest PoChM size would be 11 kilobytes for 24-hour density and 8 kilobytes for one-hour density.

# Resource Cost

24-hour posterity density:
 * Constant storage: Currently, three days of ledger data are stored, which contain about 260,000 headers. Storing as many headers requires about 2.5 megabytes.
 * Accumulated storage: An additional hash to each posterity header increases the state growth by about 11 kilobytes a year
 * Block Validation: The new step requires computing a Merkle root of a tree of height ``logN`` containing chain blocks. Since there is already fast random access to all chain blocks, the heaviest part of the computation is the number of hashes, where the required number of hashes is ``2*logN-1``. Assuming a selected chain growth rate of one block per second, this becomes 32 hashes. When there are no many reorgs this comes up to 32 hashes/second. If somehow the selected chain growth rate increases to say 10 blocks per second, this would become 39 hashes per block or 390 hashes per second. Using an efficient hash such as blake2, computing this many hashes is negligible even for very weak CPUs.

one-hour posterity density:
 * Constant storage: same as above
 * Accumulated storage: an additional 23 headers per day accumulate to about 2.3 megabytes per year
 * Block Validation: The number of hashes/second will reduce to 23 hashes/second for 1 block/second selected chain growth, or to 300 hashes/second for 10 blocks/second selected chain growth.

It is fair to say that in all scenarios, the resource costs of this upgrade are very marginal.

# Backwards compatibility
Breaks consensus rules, requires hardfork. Changes header structure.
