```
  KIP: 5
  Layer: Consensus (hard fork), Block Validation
  Title: Proofs of Validation (PoV) and Proofs of Chain Membership (PaChM)
  Author: Shai Wyborski <shai.wyborski@mail.huji.ac.il>
  Status: proposed
```

# Motivation
The pruning mechanism makes it impossible to prove that a transaction was included in the ledger after it has been pruned. The only currently available solution is publicly run archival nodes (in the form of block explorers) that store all historical data. However, this is not a sustainable solution, since the size of the databases increases rapidly with time and adoption.

A better solution is to provide a *cryptographically verifiable proof* that a transaction was posted to the blockchain. Such a proof could only be *generated* before the transaction was pruned (or by using an archival node), but could be *verified* indefinitely.

We consider two types of proof: a *proof of publication* (PoP) and a *proof of validation* (PoV). The first kind only proves that a transaction was present on the blockDAG at some point, whereas the second proves that a transaction was *accepted*. That is, while a proof of publication can prove that ``txn`` was posted to the blockchain, it can not discount the scenario where a *conflicting* transaction ``txn'`` was also posted and that ``txn'`` was validated while ``txn`` was discarded.

In this KIP I will mostly concentrate about proofs of validation. After the proposal is fully describe, I will also explain how it can be modified to obtain a slightly smaller proof of publication for scenarios where such a proof is sufficient.

The current state of the art is that it is technically possible to obtain proofs of validation, but such proofs can grow rather large (up to about 2.5 megabytes in the worst case), and there is no API call for generating such a proof. I describe a small modification to the block validation rules that extremely reduces the size of a proof of validation (down to about 9.5 kilobytes in the worst case) while incurring very mild costs on the performance of the network (in terms of header size and block validation complexity). I also provide a precise algorithmic descriptions of how such a proof is generated and verified, with the intention that they will be implemented as API calls.

I stress that the current sizes are calculated with respect to the values of various parameters in the current 1BPS consensus. Changing these parameters would require recalculating these values. However, increasing block rates will only increase the factor by which proposed proofs are improve upon currently possible proofs (roughly because currently possible proofs are as large as the entire ledger stored between two consecutive pruning blocks, whereas the size of proposed proofs grows *logarithmically* with the number of *chain blocks* between two consecutive pruning blocks. In particular, increasing BPS will increase the size of current proofs, but not the size of proposed proofs).

# Notations

In this proposal, it is convenient to use the notation ``Past(B)`` (resp. ``Future(B)``) to denote the past (resp. future) of the block ``B`` *including* the block ``B`` itself. The method names are capitalized to differentiate them from the common notations ``past(B)`` and ``future(B)`` which *exclude* the block ``B`` itself.

I use the notation ``parent(B,n)`` to note the *nth selected parent* of ``B``. For brevity, I use ``parent(B)`` instead of ``parent(B,1)``. For any ``n>1`` we can recursively define ``parent(B,n)=parent(parent(B,n-1))``.

# Stored Headers

Currently, the Kaspa node stores a block header from the selected chain once every 24 hours. The stored headers are those of previous pruning blocks, however, this is irrelevant in the context of the current proposal, so I will refer to them as `stored headers` to avoid confusion.

Stored headers have the following important properties:
 * They are determined in consensus. That is, all nodes stores the same sets of headers
 * They are headers of blocks in the *selected chain*
 * Each selected header contains a pointer to the next selected header. Hence, the chain of selected headers is verifiable all the way down to genesis. In particular, obtaining and verifying this chain is part of the process of syncing a new node.

Given a block ``B`` let ``stored_header(B)`` be the *earliest* stored header that is either ``B`` or has ``B`` in its past.

For any block ``B`` let ``next_stored_header(B)`` be the header of the block with the following property: if ``B`` is a stored header, then ``next_stored_header(B)`` is the next stored header. Note that this is well defined even if ``B`` is not a stored header.

# Accepted Transactions Merkle Root (ATMR)

In Bitcoin, every header contains the root of a Merkle tree of all transactions included in this block. In Kaspa, this Merkle tree is extended to contain all *accepted* transactions included in this block *and its merge set*. That is, all transactions that appear either in the block or the merge set of the block, except transactions that conflict other transactions that precede them in the GHOSTDAG ordering. 

# Proofs of Chain Membership (PoChM)

In order to provide a proof that ``txn`` was validated it suffices to provide the following:
 * A header of a block ``B`` and a Merkle proof that ``txn`` appears in ``B``'s (ATMR)
 * Proof that ``B`` appeared in the selected chain

This suffices to prove that ``txn`` was validated even if a conflicting transaction ``txn'`` (or any number thereof) was also included in the blockDAG: the validation rules imply that ``txn`` and ``txn'`` both appeared in the anticone of ``B``, and that ``txn`` preceded ``txn'`` in the GHOSTDAG ordering.

The first item is a straightforward Merkle proof. However, the second item is trickier. I refer to such a proof as a *proof of chain membership* (PoChM, pronounced like the Latin word *pacem*) for ``B``. The rest of this document is concerned with providing a PoChM for an arbitrary block ``B``.

# PoChM Without Hard-Fork

Currently, the most straightforward way to construct a PoChM for ``B`` is to store the entire set ``Future(B) ∩ Past(stored_block(B))``. The result is a "diamond shaped" DAG whose only source is ``stored_block(B)`` and only sink is ``B``. Given a DAG of this shape as proof, any node could verify that its source is a known stored header, that its sink is *B*, and that starting with the source and following the selected parents leads to the sink.

The problem with this proof is its size. In the worst case, it would be about as large as 24 hours worth of headers. At the current 1BPS and header size of 248 bytes, this sums to about 2.5 megabytes.

Remark: This could be improved slightly by, instead of storing the entire set, only storing the headers of the selected chain blocks and their parents. This data suffices to compute the selected parent of each selected chain block and validate the proof. However, this does not seem to improve the size by much. Also note that proofs for many transactions that were accepted in chain blocks with the same ``stored_block`` can be aggregated. In particular, a PoChM for a block B is also a PoChM for any chain block ``C ∈ Future(B) ∩ Past(stored_block(B))``

# Performance trade-offs

Our goal is to decrease the size of a PoChM as much as possible while minimizing performance costs. There are two relevant types of costs: header sizes, and block validation complexity.

As an extreme example, one could provide a very small PoChM by including in each stored header the entire list of headers of all chain blocks down to the next stored header. This "solution" is obviously prohibitive as it will make block headers huge.

On the other extreme, one could include in each header a *Merkle tree* of all chain blocks down to the next stored header, and let the PoChM of B be a Merkle proof for that tree. While this "solution" only increases the size of a block header by 32 bytes (the size of a single hash), it makes it necessary to compute tens of thousands of hashes to validate a single block header, which is prohibitively costly.

Our proposal "balances" the second approach: we add to each header the root of a Merkle tree that only contains *logarithmically many* headers, this allows to generate a PoChM in the form of a logarithmically long chain of headers and Merkle proofs.

In the current parametrization, implementing our propoasl requires increasing the size of a header by a single hash (32 bytes), and adds a validation step with constant space complexity and a time complexity of θ(log(N)) where N is the number of *chain* blocks between two consecutive stored blocks. The size of a PoChM, as well as the time required to verify it, is θ(log(N)loglog(N)).

We will provide non-asymptotic bounds after specifying the solution. For now we will state that in the current parametrization, the new block validation step requires computing 33 hashes (and can be skipped for blocks outside the selected chain), and that, in the worst case, the size of a PoChM is about 9.5 kilobytes.

# Uniform Validation Complexity

In theory, we could have defined our result such that the new validation step only applies at blocks chosen to be stored blocks. However, this makes stored blocks harder to construct and verify compared to other blocks. This might significantly increase their propagation time, decentivize miners from mining them, or have other unexpected adverse effects. To avoid this, we propose requiring all blocks to pass this verification step.

# Our Proposal

The block header will contain a new field called the *PoChM Merkle root* (PMR) defined as follows: let k be the least integer such that ``parent(B,2^k) ∈ Past(next_stored_header(B))``, then PMR is the root of the Merkle tree containing the headers ``parent(B,2^i)`` for ``i = 0,...,k-1``.

The process of header validation of chain block candidates will include verifying the PMR. I propose that the PMR will not be validated for blocks that are not chain candidates. In particular, a block whose PMR is invalid but is otherwise valid will remain in the DAG but will be disqualified from being a selected tip/parent. A similar approach is employed when validating (can't remember what that is, remind myself and add a link).

will be added to the block header. The PMR of ``B`` will be the Merkle root of a tree containing hashes of the following headers parent(B,

The procedure for computing ``B.window(size,sparsity)`` is as follows:
     
    function complete_window(m,B):
        let i = 0
        for C in B.merge_set:
            if C.blue_score < B.blue_score - length*BPS:
                mark C as non-DAA
                continue
            i++
            if i + B.selected_parent.DAA_score % sparsity == 0:
                m.push(C)
     
    if B.selected_parent is cached:
        B.window(size,sparsity) = complete_window(B,B.selected_parent.window(size,sparsity).copy())
    else:
        B.window(size,sparsity) = new bounded_min_heap(bound = size)
        C = B
        while B.window(size,sparsity).size() < size OR C.blue_work >= B.window(size,sparsity).min():
            B.window(size,sparsity) = complete_window(C,B.window(size,sparsity))
            C = C.selected_parent
            
**Remarks**:
 * The window of the cached selected parent is copied and not modified in place, as it might be needed for computing the windows of several blocks
 * The computation process in the non-cached case goes back in the chain, but it produces the same result as incrementally updating a cached window due to the strong monotonicity of DAA_score.

It remains to specify the ordering in which B.merge_set is traversed, we propose using blue_work in *descending* order. That is, starting with the highest blue_work. This way the ordering of the merge set depends on the future, making it harder to game.

# Backwards compatibility
Breaks consensus rules, requires hardfork. Changes header structure.
